Lawin(
  (encoder): MixTransformer(
    (layers): ModuleList(
      (0): TransformerBlock(
        (patch): OverlapPatchEmbeding(
          (projection): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        )
        (layers): ModuleList(
          (0): TransformerLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=64, out_features=64, bias=True)
                (in_proj_k): Linear(in_features=64, out_features=64, bias=True)
                (in_proj_v): Linear(in_features=64, out_features=64, bias=True)
                (out_proj): Linear(in_features=64, out_features=64, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.0)
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=64, bias=True)
            )
            (drop2): DropPath(drop_prob=0.0)
          )
          (1): TransformerLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=64, out_features=64, bias=True)
                (in_proj_k): Linear(in_features=64, out_features=64, bias=True)
                (in_proj_v): Linear(in_features=64, out_features=64, bias=True)
                (out_proj): Linear(in_features=64, out_features=64, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.006666666828095913)
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=64, bias=True)
            )
            (drop2): DropPath(drop_prob=0.006666666828095913)
          )
          (2): TransformerLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=64, out_features=64, bias=True)
                (in_proj_k): Linear(in_features=64, out_features=64, bias=True)
                (in_proj_v): Linear(in_features=64, out_features=64, bias=True)
                (out_proj): Linear(in_features=64, out_features=64, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.013333333656191826)
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=64, bias=True)
            )
            (drop2): DropPath(drop_prob=0.013333333656191826)
          )
        )
        (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      )
      (1): TransformerBlock(
        (patch): OverlapPatchEmbeding(
          (projection): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        )
        (layers): ModuleList(
          (0): TransformerLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=128, out_features=128, bias=True)
                (in_proj_k): Linear(in_features=128, out_features=128, bias=True)
                (in_proj_v): Linear(in_features=128, out_features=128, bias=True)
                (out_proj): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.019999999552965164)
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              (act): GELU()
              (fc2): Linear(in_features=512, out_features=128, bias=True)
            )
            (drop2): DropPath(drop_prob=0.019999999552965164)
          )
          (1): TransformerLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=128, out_features=128, bias=True)
                (in_proj_k): Linear(in_features=128, out_features=128, bias=True)
                (in_proj_v): Linear(in_features=128, out_features=128, bias=True)
                (out_proj): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.02666666731238365)
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              (act): GELU()
              (fc2): Linear(in_features=512, out_features=128, bias=True)
            )
            (drop2): DropPath(drop_prob=0.02666666731238365)
          )
          (2): TransformerLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=128, out_features=128, bias=True)
                (in_proj_k): Linear(in_features=128, out_features=128, bias=True)
                (in_proj_v): Linear(in_features=128, out_features=128, bias=True)
                (out_proj): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.03333333507180214)
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              (act): GELU()
              (fc2): Linear(in_features=512, out_features=128, bias=True)
            )
            (drop2): DropPath(drop_prob=0.03333333507180214)
          )
          (3): TransformerLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=128, out_features=128, bias=True)
                (in_proj_k): Linear(in_features=128, out_features=128, bias=True)
                (in_proj_v): Linear(in_features=128, out_features=128, bias=True)
                (out_proj): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.03999999910593033)
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              (act): GELU()
              (fc2): Linear(in_features=512, out_features=128, bias=True)
            )
            (drop2): DropPath(drop_prob=0.03999999910593033)
          )
        )
        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
      (2): TransformerBlock(
        (patch): OverlapPatchEmbeding(
          (projection): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        )
        (layers): ModuleList(
          (0): TransformerLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=320, out_features=320, bias=True)
                (in_proj_k): Linear(in_features=320, out_features=320, bias=True)
                (in_proj_v): Linear(in_features=320, out_features=320, bias=True)
                (out_proj): Linear(in_features=320, out_features=320, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.046666666865348816)
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              (act): GELU()
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
            )
            (drop2): DropPath(drop_prob=0.046666666865348816)
          )
          (1): TransformerLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=320, out_features=320, bias=True)
                (in_proj_k): Linear(in_features=320, out_features=320, bias=True)
                (in_proj_v): Linear(in_features=320, out_features=320, bias=True)
                (out_proj): Linear(in_features=320, out_features=320, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.0533333346247673)
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              (act): GELU()
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
            )
            (drop2): DropPath(drop_prob=0.0533333346247673)
          )
          (2): TransformerLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=320, out_features=320, bias=True)
                (in_proj_k): Linear(in_features=320, out_features=320, bias=True)
                (in_proj_v): Linear(in_features=320, out_features=320, bias=True)
                (out_proj): Linear(in_features=320, out_features=320, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.06000000238418579)
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              (act): GELU()
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
            )
            (drop2): DropPath(drop_prob=0.06000000238418579)
          )
          (3): TransformerLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=320, out_features=320, bias=True)
                (in_proj_k): Linear(in_features=320, out_features=320, bias=True)
                (in_proj_v): Linear(in_features=320, out_features=320, bias=True)
                (out_proj): Linear(in_features=320, out_features=320, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.06666667014360428)
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              (act): GELU()
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
            )
            (drop2): DropPath(drop_prob=0.06666667014360428)
          )
          (4): TransformerLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=320, out_features=320, bias=True)
                (in_proj_k): Linear(in_features=320, out_features=320, bias=True)
                (in_proj_v): Linear(in_features=320, out_features=320, bias=True)
                (out_proj): Linear(in_features=320, out_features=320, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.07333333790302277)
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              (act): GELU()
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
            )
            (drop2): DropPath(drop_prob=0.07333333790302277)
          )
          (5): TransformerLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (reduction): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=320, out_features=320, bias=True)
                (in_proj_k): Linear(in_features=320, out_features=320, bias=True)
                (in_proj_v): Linear(in_features=320, out_features=320, bias=True)
                (out_proj): Linear(in_features=320, out_features=320, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.07999999821186066)
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              (act): GELU()
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
            )
            (drop2): DropPath(drop_prob=0.07999999821186066)
          )
        )
        (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      )
      (3): TransformerBlock(
        (patch): OverlapPatchEmbeding(
          (projection): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
        (layers): ModuleList(
          (0): TransformerLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=512, out_features=512, bias=True)
                (in_proj_k): Linear(in_features=512, out_features=512, bias=True)
                (in_proj_v): Linear(in_features=512, out_features=512, bias=True)
                (out_proj): Linear(in_features=512, out_features=512, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.08666667342185974)
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              (act): GELU()
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
            )
            (drop2): DropPath(drop_prob=0.08666667342185974)
          )
          (1): TransformerLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=512, out_features=512, bias=True)
                (in_proj_k): Linear(in_features=512, out_features=512, bias=True)
                (in_proj_v): Linear(in_features=512, out_features=512, bias=True)
                (out_proj): Linear(in_features=512, out_features=512, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.09333333373069763)
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              (act): GELU()
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
            )
            (drop2): DropPath(drop_prob=0.09333333373069763)
          )
          (2): TransformerLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj_q): Linear(in_features=512, out_features=512, bias=True)
                (in_proj_k): Linear(in_features=512, out_features=512, bias=True)
                (in_proj_v): Linear(in_features=512, out_features=512, bias=True)
                (out_proj): Linear(in_features=512, out_features=512, bias=True)
              )
            )
            (drop1): DropPath(drop_prob=0.10000000149011612)
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              (act): GELU()
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
            )
            (drop2): DropPath(drop_prob=0.10000000149011612)
          )
        )
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (decoder): LawinHead(
    (mlp_layers): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU(inplace=True)
      )
      (3): ConvModule(
        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU(inplace=True)
      )
    )
    (mlp1): ConvModule(
      (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act): ReLU(inplace=True)
    )
    (lawin_attns): ModuleList(
      (0): LargeWindowAttention(
        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (position_mixing): PositionMixing(
          (mixers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
          )
        )
        (attn): MultiheadAttention(
          (in_proj_q): Linear(in_features=128, out_features=128, bias=True)
          (in_proj_k): Linear(in_features=128, out_features=128, bias=True)
          (in_proj_v): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (1): LargeWindowAttention(
        (pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (position_mixing): PositionMixing(
          (mixers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
            (4): Linear(in_features=64, out_features=64, bias=True)
            (5): Linear(in_features=64, out_features=64, bias=True)
            (6): Linear(in_features=64, out_features=64, bias=True)
            (7): Linear(in_features=64, out_features=64, bias=True)
            (8): Linear(in_features=64, out_features=64, bias=True)
            (9): Linear(in_features=64, out_features=64, bias=True)
            (10): Linear(in_features=64, out_features=64, bias=True)
            (11): Linear(in_features=64, out_features=64, bias=True)
            (12): Linear(in_features=64, out_features=64, bias=True)
            (13): Linear(in_features=64, out_features=64, bias=True)
            (14): Linear(in_features=64, out_features=64, bias=True)
            (15): Linear(in_features=64, out_features=64, bias=True)
          )
        )
        (attn): MultiheadAttention(
          (in_proj_q): Linear(in_features=128, out_features=128, bias=True)
          (in_proj_k): Linear(in_features=128, out_features=128, bias=True)
          (in_proj_v): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (2): LargeWindowAttention(
        (pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (position_mixing): PositionMixing(
          (mixers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): Linear(in_features=64, out_features=64, bias=True)
            (4): Linear(in_features=64, out_features=64, bias=True)
            (5): Linear(in_features=64, out_features=64, bias=True)
            (6): Linear(in_features=64, out_features=64, bias=True)
            (7): Linear(in_features=64, out_features=64, bias=True)
            (8): Linear(in_features=64, out_features=64, bias=True)
            (9): Linear(in_features=64, out_features=64, bias=True)
            (10): Linear(in_features=64, out_features=64, bias=True)
            (11): Linear(in_features=64, out_features=64, bias=True)
            (12): Linear(in_features=64, out_features=64, bias=True)
            (13): Linear(in_features=64, out_features=64, bias=True)
            (14): Linear(in_features=64, out_features=64, bias=True)
            (15): Linear(in_features=64, out_features=64, bias=True)
            (16): Linear(in_features=64, out_features=64, bias=True)
            (17): Linear(in_features=64, out_features=64, bias=True)
            (18): Linear(in_features=64, out_features=64, bias=True)
            (19): Linear(in_features=64, out_features=64, bias=True)
            (20): Linear(in_features=64, out_features=64, bias=True)
            (21): Linear(in_features=64, out_features=64, bias=True)
            (22): Linear(in_features=64, out_features=64, bias=True)
            (23): Linear(in_features=64, out_features=64, bias=True)
            (24): Linear(in_features=64, out_features=64, bias=True)
            (25): Linear(in_features=64, out_features=64, bias=True)
            (26): Linear(in_features=64, out_features=64, bias=True)
            (27): Linear(in_features=64, out_features=64, bias=True)
            (28): Linear(in_features=64, out_features=64, bias=True)
            (29): Linear(in_features=64, out_features=64, bias=True)
            (30): Linear(in_features=64, out_features=64, bias=True)
            (31): Linear(in_features=64, out_features=64, bias=True)
            (32): Linear(in_features=64, out_features=64, bias=True)
            (33): Linear(in_features=64, out_features=64, bias=True)
            (34): Linear(in_features=64, out_features=64, bias=True)
            (35): Linear(in_features=64, out_features=64, bias=True)
            (36): Linear(in_features=64, out_features=64, bias=True)
            (37): Linear(in_features=64, out_features=64, bias=True)
            (38): Linear(in_features=64, out_features=64, bias=True)
            (39): Linear(in_features=64, out_features=64, bias=True)
            (40): Linear(in_features=64, out_features=64, bias=True)
            (41): Linear(in_features=64, out_features=64, bias=True)
            (42): Linear(in_features=64, out_features=64, bias=True)
            (43): Linear(in_features=64, out_features=64, bias=True)
            (44): Linear(in_features=64, out_features=64, bias=True)
            (45): Linear(in_features=64, out_features=64, bias=True)
            (46): Linear(in_features=64, out_features=64, bias=True)
            (47): Linear(in_features=64, out_features=64, bias=True)
            (48): Linear(in_features=64, out_features=64, bias=True)
            (49): Linear(in_features=64, out_features=64, bias=True)
            (50): Linear(in_features=64, out_features=64, bias=True)
            (51): Linear(in_features=64, out_features=64, bias=True)
            (52): Linear(in_features=64, out_features=64, bias=True)
            (53): Linear(in_features=64, out_features=64, bias=True)
            (54): Linear(in_features=64, out_features=64, bias=True)
            (55): Linear(in_features=64, out_features=64, bias=True)
            (56): Linear(in_features=64, out_features=64, bias=True)
            (57): Linear(in_features=64, out_features=64, bias=True)
            (58): Linear(in_features=64, out_features=64, bias=True)
            (59): Linear(in_features=64, out_features=64, bias=True)
            (60): Linear(in_features=64, out_features=64, bias=True)
            (61): Linear(in_features=64, out_features=64, bias=True)
            (62): Linear(in_features=64, out_features=64, bias=True)
            (63): Linear(in_features=64, out_features=64, bias=True)
          )
        )
        (attn): MultiheadAttention(
          (in_proj_q): Linear(in_features=128, out_features=128, bias=True)
          (in_proj_k): Linear(in_features=128, out_features=128, bias=True)
          (in_proj_v): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
    (image_pooling): Sequential(
      (0): AdaptiveAvgPool2d(output_size=1)
      (1): ConvModule(
        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU(inplace=True)
      )
    )
    (mlp2): ConvModule(
      (conv): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act): ReLU(inplace=True)
    )
    (mlp3): ConvModule(
      (conv): Conv2d(304, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act): ReLU(inplace=True)
    )
    (drop): Dropout2d(p=0.1, inplace=False)
    (seg_top): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))
  )
  (seg_loss): CrossEntropyLoss()
)
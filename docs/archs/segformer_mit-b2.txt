SegFormer(
  (encoder): MixTransformer(
    (layers): ModuleList(
      (0): TransformerBlock(
        (patch): OverlapPatchEmbeding(
          (projection): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        )
        (layers): ModuleList(
          (0): TransformerLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=64, out_features=192, bias=True)
                (out_proj): Linear(in_features=64, out_features=64, bias=True)
              )
              (reduction): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.0)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop_path): DropPath(drop_prob=0.0)
            )
          )
          (1): TransformerLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=64, out_features=192, bias=True)
                (out_proj): Linear(in_features=64, out_features=64, bias=True)
              )
              (reduction): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.006666666828095913)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop_path): DropPath(drop_prob=0.006666666828095913)
            )
          )
          (2): TransformerLayer(
            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=64, out_features=192, bias=True)
                (out_proj): Linear(in_features=64, out_features=64, bias=True)
              )
              (reduction): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.013333333656191826)
            )
            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=64, out_features=256, bias=True)
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=64, bias=True)
              (drop_path): DropPath(drop_prob=0.013333333656191826)
            )
          )
        )
        (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      )
      (1): TransformerBlock(
        (patch): OverlapPatchEmbeding(
          (projection): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        )
        (layers): ModuleList(
          (0): TransformerLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=128, out_features=384, bias=True)
                (out_proj): Linear(in_features=128, out_features=128, bias=True)
              )
              (reduction): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.019999999552965164)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              (act): GELU()
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop_path): DropPath(drop_prob=0.019999999552965164)
            )
          )
          (1): TransformerLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=128, out_features=384, bias=True)
                (out_proj): Linear(in_features=128, out_features=128, bias=True)
              )
              (reduction): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.02666666731238365)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              (act): GELU()
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop_path): DropPath(drop_prob=0.02666666731238365)
            )
          )
          (2): TransformerLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=128, out_features=384, bias=True)
                (out_proj): Linear(in_features=128, out_features=128, bias=True)
              )
              (reduction): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.03333333507180214)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              (act): GELU()
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop_path): DropPath(drop_prob=0.03333333507180214)
            )
          )
          (3): TransformerLayer(
            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=128, out_features=384, bias=True)
                (out_proj): Linear(in_features=128, out_features=128, bias=True)
              )
              (reduction): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.03999999910593033)
            )
            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              (act): GELU()
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop_path): DropPath(drop_prob=0.03999999910593033)
            )
          )
        )
        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      )
      (2): TransformerBlock(
        (patch): OverlapPatchEmbeding(
          (projection): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        )
        (layers): ModuleList(
          (0): TransformerLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=320, out_features=960, bias=True)
                (out_proj): Linear(in_features=320, out_features=320, bias=True)
              )
              (reduction): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.046666666865348816)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              (act): GELU()
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop_path): DropPath(drop_prob=0.046666666865348816)
            )
          )
          (1): TransformerLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=320, out_features=960, bias=True)
                (out_proj): Linear(in_features=320, out_features=320, bias=True)
              )
              (reduction): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.0533333346247673)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              (act): GELU()
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop_path): DropPath(drop_prob=0.0533333346247673)
            )
          )
          (2): TransformerLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=320, out_features=960, bias=True)
                (out_proj): Linear(in_features=320, out_features=320, bias=True)
              )
              (reduction): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.06000000238418579)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              (act): GELU()
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop_path): DropPath(drop_prob=0.06000000238418579)
            )
          )
          (3): TransformerLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=320, out_features=960, bias=True)
                (out_proj): Linear(in_features=320, out_features=320, bias=True)
              )
              (reduction): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.06666667014360428)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              (act): GELU()
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop_path): DropPath(drop_prob=0.06666667014360428)
            )
          )
          (4): TransformerLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=320, out_features=960, bias=True)
                (out_proj): Linear(in_features=320, out_features=320, bias=True)
              )
              (reduction): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.07333333790302277)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              (act): GELU()
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop_path): DropPath(drop_prob=0.07333333790302277)
            )
          )
          (5): TransformerLayer(
            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=320, out_features=960, bias=True)
                (out_proj): Linear(in_features=320, out_features=320, bias=True)
              )
              (reduction): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.07999999821186066)
            )
            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=320, out_features=1280, bias=True)
              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
              (act): GELU()
              (fc2): Linear(in_features=1280, out_features=320, bias=True)
              (drop_path): DropPath(drop_prob=0.07999999821186066)
            )
          )
        )
        (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      )
      (3): TransformerBlock(
        (patch): OverlapPatchEmbeding(
          (projection): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        )
        (layers): ModuleList(
          (0): TransformerLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=512, out_features=1536, bias=True)
                (out_proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.08666667342185974)
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              (act): GELU()
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop_path): DropPath(drop_prob=0.08666667342185974)
            )
          )
          (1): TransformerLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=512, out_features=1536, bias=True)
                (out_proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.09333333373069763)
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              (act): GELU()
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop_path): DropPath(drop_prob=0.09333333373069763)
            )
          )
          (2): TransformerLayer(
            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (attn): EfficientSelfAttention(
              (attn): MultiheadAttention(
                (in_proj): Linear(in_features=512, out_features=1536, bias=True)
                (out_proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.10000000149011612)
            )
            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (ffn): MixFFN(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              (act): GELU()
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop_path): DropPath(drop_prob=0.10000000149011612)
            )
          )
        )
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (decoder): SegFormerHead(
    (mlp_layers): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU(inplace=True)
      )
      (3): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU(inplace=True)
      )
    )
    (mlp): ConvModule(
      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act): ReLU(inplace=True)
    )
    (drop): Dropout2d(p=0.1, inplace=False)
    (seg_top): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))
  )
  (seg_loss): CrossEntropyLoss()
)